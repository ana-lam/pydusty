#!/usr/bin/env python

# My Python re-write of Kochanek's dustymc2
# Requires the following in a directory to run properly:
#    dusty
#    dusty.inp
#    lambda_grid.dat
# Change Log:
#   16 Oct 2014 - forking from dusty_wrapper.py to add capability of dealing with only upper limits
#		eventually I'd like to replace fitpoly.py
#   14 Jan 2015 - added tau_eff_coeff to header
#   17 Apr 2015 - changed to loop through luminosities rather than find chi_limits=4
#   27 Sep 2017 - fixed bug in calculate_tau_coeff

import numpy as np
import subprocess
from numpy.random import normal as gasdev
from numpy.random import uniform as rand
from sys import exit, argv
from time import time
from datetime import datetime
import argparse
from os import system
# Non-standard modules
from astropy.io import ascii
import jdcal

def calculate_tau_coeff():
  coefficient_data = ascii.read('foo1.stb')
  # find the wavelength closest to that of dLdt_filter
  lambda_diff = np.abs(coefficient_data['col1'] - args.effective_optical_depth_lambda)
  index = np.argmin(lambda_diff)
  if lambda_diff[index] > 0.01:
    print 'ERROR: effective_optical_depth_lambda not contained within foo1.stb'
    exit(1)
  albedo = coefficient_data['col8'][index]
  tau_coeff = (1.0 - albedo)**0.5
  if verbose:
    print 'albedo = %s' % (albedo)
    print 'tau_eff coefficient = %s' % (tau_coeff)
  # calculate tau_effective ratios
  tau_ratios = []
  tau_optical = coefficient_data['col7'][index]*tau_coeff
  if args.variability_constraints or args.dLdt_limit:
    for wavelength in dLdt_lambdas:
      tmp_lambda_diff = np.abs(coefficient_data['col1'] - wavelength)
      tmp_index = np.argmin(tmp_lambda_diff)
      if tmp_lambda_diff[tmp_index] > 0.1:
        print 'ERROR: %0.2f microns lies outside of foo1.stb spectrum' (wavelength)
        exit(1)
      tmp_albedo = coefficient_data['col8'][tmp_index]
      tmp_tau_coeff = (1.0 - tmp_albedo)**0.5
      tau_wavelength = coefficient_data['col7'][tmp_index]*tmp_tau_coeff
      tau_ratio = tau_wavelength / tau_optical
      tau_ratios.append(tau_ratio)
      if verbose:
        print 'tau_eff(%0.2f-micron)/tau_eff(%0.2f-micron) = %0.2f' % (wavelength,args.effective_optical_depth_lambda,tau_ratio)
  return tau_coeff, np.array(tau_ratios)

def readkurucz():
  flist = []
  # these are generated by SMOOTH2.PL in MARCS
  flist.append(path_to_filter_files+'kuruczt3500g40k2od.dat') 
  flist.append(path_to_filter_files+'kuruczt3750g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt4000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt4250g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt4500g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt4750g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt5000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt5250g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt5500g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt5750g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt6000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt6250g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt6500g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt6750g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt7000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt7250g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt7500g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt7750g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt8000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt8250g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt8500g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt8750g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt9000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt9250g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt9500g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt9750g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt10000g40k2od.dat') 
  flist.append(path_to_filter_files+'kuruczt10250g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt10500g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt10750g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt11000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt11250g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt11500g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt11750g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt12000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt12250g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt12500g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt12750g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt13000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt14000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt15000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt16000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt17000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt18000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt19000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt20000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt21000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt22000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt23000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt24000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt25000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt26000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt27000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt28000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt29000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt30000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt31000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt32000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt33000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt34000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt35000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt36000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt37000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt38000g40k2od.dat')
  flist.append(path_to_filter_files+'kuruczt39000g40k2od.dat')
  tlist = []
  tlist.append(3500) 
  tlist.append(3750)
  tlist.append(4000)
  tlist.append(4250)
  tlist.append(4500)
  tlist.append(4750)
  tlist.append(5000)
  tlist.append(5250)
  tlist.append(5500)
  tlist.append(5750)
  tlist.append(6000) 
  tlist.append(6250)
  tlist.append(6500)
  tlist.append(6750)
  tlist.append(7000)
  tlist.append(7250)
  tlist.append(7500)
  tlist.append(7750)
  tlist.append(8000)
  tlist.append(8250)
  tlist.append(8500)
  tlist.append(8750)
  tlist.append(9000)
  tlist.append(9250)
  tlist.append(9500)
  tlist.append(9750)
  tlist.append(10000) 
  tlist.append(10250)
  tlist.append(10500)
  tlist.append(10750)
  tlist.append(11000)
  tlist.append(11250)
  tlist.append(11500)
  tlist.append(11750)
  tlist.append(12000)
  tlist.append(12250)
  tlist.append(12500)
  tlist.append(12750)
  tlist.append(13000)
  tlist.append(14000)
  tlist.append(15000)
  tlist.append(16000)
  tlist.append(17000)
  tlist.append(18000)
  tlist.append(19000)
  tlist.append(20000)
  tlist.append(21000)
  tlist.append(22000)
  tlist.append(23000)
  tlist.append(24000)
  tlist.append(25000)
  tlist.append(26000)
  tlist.append(27000)
  tlist.append(28000)
  tlist.append(29000)
  tlist.append(30000)
  tlist.append(31000)
  tlist.append(32000)
  tlist.append(33000)
  tlist.append(34000)
  tlist.append(35000)
  tlist.append(36000)
  tlist.append(37000)
  tlist.append(38000)
  tlist.append(39000)
  nlist = len(tlist)
  if verbose: print 'read Kurucz model file list'
  return nlist, tlist, flist

def read_filters(filters):
  "read filter response curve for each input filter"
  trans = []
  # first verify that I know what each filter is
  recognized_filters = np.array(['WFC3uvF275W','WFC3uvF336W','WFC3uvF438W','WFC3uvF475W','WFC3uvF555W','WFC3uvF606W','WFC3uvF814W','WFC3irF110W','WFC3irF160W','WFPC2F555W','WFPC2F814W','ACSWFCF435W','ACSWFCF475W','ACSWFCF555W','ACSWFCF606W','ACSWFCF814W','F36','F45','F58','F80','V','I','J','H','lbcrY','lbcrSDSSz','lbcrSDSSr','lbcrSDSSi','lbcrBesselV','lbcrBesselR','lbcrBesselI','lbcbSDSSr','lbcbSDSSg','lbcbUspec','lbcbBesselV','lbcbBesselU','lbcbBesselB','R','F24','F70','Ks'])
  for filter in filters:
    match_filter = (recognized_filters == filter)
    if len(recognized_filters[match_filter]) == 1:
      filter_file = path_to_filter_files + 'filt' + recognized_filters[match_filter][0] + '.dat'
    else:
      print 'ERROR: unsupported filter: %s' % (filter)
      print '  this is the list of currently supported filters:'
      for sfilter in recognized_filters:
        print '\t\t%s' % (sfilter)
      exit(1)
    # now read in the filter transmission curve
    templam, trans1 = np.loadtxt(filter_file, usecols=(0,1), unpack=True)
    trans.append(trans1)
  return templam, trans

def filter_effective_wavelength(filters):
  "return effective wavelengths for filters"
  lambdas = {}
  lambdas['U'] = 0.36   # microns
  lambdas['B'] = 0.44
  lambdas['V'] = 0.55
  lambdas['R'] = 0.658
  lambdas['WFC3uvF555W'] = 0.51874
  lambdas['WFC3uvF814W'] = 0.79011
  lambdas['WFC3irF110W'] = 1.15340
  lambdas['WFC3irF160W'] = 1.53690
  lambda_eff = []
  for filter in filters:
    try:
      lambda_eff.append(lambdas[filter])
    except:
      print 'ERROR: %s-band effective wavelength not found.  This must be added to the code.' % (filter)
      exit(1)
  return np.array(lambda_eff)

def geninput(tstar,tau,td,thick,idtype,fileuse):
  output = open('foo1.inp','w')
  output.write('Spectrum = 5   \n')
  output.write('%s\n' % (fileuse))
  output.write('   optical properties index = 1 \n')
  output.write('   #   Sil-Ow  Sil-Oc  Sil-DL  grf-DL  amC-Hn  SiC-Pg \n')
  if idtype == 0:
    output.write('    x = 0.00    0.00   0.00    1.00    0.00    0.00 \n')
  else:
    output.write('    x = 0.00    0.00   1.00    0.00    0.00    0.00 \n')
  if custom_grain_distribution:
    output.write('- size distribution = 2  % custom       \n')
    output.write('  q = 3.5, a(min) = %s micron, a(max) = %s micron\n' % (amin,amax))
  else:
    output.write('- size distribution = 1  % standard MRN    \n')
  output.write('- temperature = %s K \n' % (td))
  output.write('- density type = 1                   \n')
  output.write('- number of powers = 1              \n')
  output.write('- shells relative thickness = %s\n' % (thick))
  output.write('- power = 2 \n')
  output.write('- grid type = 1                  % linear grid \n')
  output.write('- lambda0 = 0.55 micron          % optical depth specified  \n')
  output.write('- tau(min) = '+str(tau)+' ; tau(max) = 1000.0   % for the visual wavelength \n')
  output.write('- number of models = 1           \n')
  output.write('- accuracy for flux conservation = 0.05             \n')
  output.write('- verbosity flag;                              verbose = 1  \n')
  output.write('- properties of emerging spectra;            fname.spp = 1  \n')
  output.write('- detailed spectra for each model;          fname.s### = 1  \n')
  output.write('- images at specified wavelengths;          fname.i### = 1  \n')
  output.write('     number of wavelengths = 5  \n')
  output.write('     wavelengths = 3.5, 4.5, 6.0, 8.0, 24.0 micron  \n')
  output.write('- radial profiles for each model;           fname.r### = 1  \n')
  output.write('- detailed run-time messages;               fname.m### = 1  \n')
  output.write('- visibility function at spec. wavelengths; fname.v### = 0  \n')
  output.close()

  if verbose: print 'calling dusty with tstar = %0.1f; tau = %0.1f; td = %0.1f; thick %0.1f' % (tstar,tau,td,thick)
  #system('./dusty')
  process = subprocess.Popen(["./dusty"], shell=True, stderr=subprocess.STDOUT,stdout=subprocess.PIPE)
  output,err = process.communicate()
  if err:
    print 'ERROR while running DUSTY:', err
    exit(1)

  ierror = 0
  #try:
  data = ascii.read('foo1.stb')
  lam = data['col1']
  flx = data['col2']
  npt = len(lam)

  i = 0
  with open('foo1.out','r') as f:
    for line in f:
      if i == 42:
        # print 'line read in from foo1.out:', line
        line_s = line.split()
        id = int(line_s[0])
        tau0 = float(line_s[1])
        f1 = float(line_s[2])
        r1 = float(line_s[3])
        r1torstar = float(line_s[4])
        theta1 = float(line_s[5])
        tdout = float(line_s[6])
        break
      i += 1   
  #except:
  #  ierror = 1
  #  lam = np.nan
  #  flx = np.nan
  #  npt = np.nan
  #  r1 = np.nan
  return lam, flx, npt, r1, ierror

def locate(xx,n,x):
  jl = 0
  ju = n+1
  while ju - jl > 1:
    jm = (ju+jl) / 2
    if (xx[n-1] > xx[0] and x > xx[jm-1] ) or (xx[n-1] < xx[0] and x < xx[jm-1]):
      jl = jm
    else:
      ju = jm
  j = jl
  return j    

#----------------------------------------------------------------------
# routines specific to the limits-only case
def fit_parabola(x,y,value):
  "fit parabola to x, y-values, return y_fit(value)"
  z3 = np.polyfit(x,y,3)        # fit 3rd-deg polynomial
  p3 = np.poly1d(z3)
  return p3(value)

def optimize_tdust(tstarnew,taunew,thicknew):
  "Run Dusty with two different tdust values.  Then repeat with different tdust values until v_out values bracket v_in with a minimum of 3 trials (so a parabola can be fit)"
  tol = 0.01    # tolerance for final chi^2
  maxtries = 5
  tdust_list = [100, 300]	# intial tdust guesses
  v_list = []
  # run dusty for first test point
  chi,tstar_out,tau_out,td_out,thick_out,sluml,r1,vlog,dustm, chi_dLdt,nvarpoints,chi_vlog = dusty_loop(tstarnew,tdust_list[0],taunew,thicknew)
  if taunew == 0:	# if tau = 0, adjusting the dust temperature is meaningless
    return chi, tstar_out, tau_out, td_out, thick_out, sluml, r1, vlog, dustm, chi_dLdt,nvarpoints,chi_vlog
  v_list.append(vlog)
  # run dusty for second test point
  chi,tstar_out,tau_out,td_out,thick_out,sluml,r1,vlog,dustm, chi_dLdt,nvarpoints,chi_vlog = dusty_loop(tstarnew,tdust_list[1],taunew,thicknew)
  v_list.append(vlog)
  # based on results of first two test points, do a third test point
  vmin = min(v_list)
  vmax = max(v_list)
  vtoosmall = 0
  vtoobig = 0
  if vmin > vlog0:
    chi,tstar_out,tau_out,td_out,thick_out,sluml,r1,vlog,dustm, chi_dLdt,nvarpoints,chi_vlog = dusty_loop(tstarnew,400,taunew,thicknew)
    v_list.append(vlog)
    tdust_list.append(400)
  elif vmax < vlog0:
    chi,tstar_out,tau_out,td_out,thick_out,sluml,r1,vlog,dustm, chi_dLdt,nvarpoints,chi_vlog = dusty_loop(tstarnew,50,taunew,thicknew)
    v_list.append(vlog)
    tdust_list.append(50)
  else:
    chi,tstar_out,tau_out,td_out,thick_out,sluml,r1,vlog,dustm, chi_dLdt,nvarpoints,chi_vlog = dusty_loop(tstarnew,200,taunew,thicknew)
    v_list.append(vlog)
    tdust_list.append(200)
  # while the test points do not yet bracket vgoal, do additional tests with different tdust values
  Tmin = min(tdust_list)
  Tmax = max(tdust_list)
  while vmin > vlog0:
    vtoobig += 1
    T_in = Tmax*2
    if verbose: print 'vtoobig = %d, T_in = %d' % (vtoobig,T_in)
    chi,tstar_out,tau_out,td_out,thick_out,sluml,r1,vlog,dustm, chi_dLdt,nvarpoints,chi_vlog = dusty_loop(tstarnew,T_in,taunew,thicknew)
    v_list.append(vlog)
    tdust_list.append(T_in)
    if verbose: print '    vlast =', v_list[-1], 'vgoal =', vlog0
    vmin = min(v_list)
    vmax = max(v_list)
    Tmin = min(tdust_list)
    Tmax = max(tdust_list)
    if vtoobig > 5: break
  while vmax < vlog0:
    vtoosmall += 1
    T_in = Tmin*0.7
    if verbose: print 'vtoosmall = %d, T_in = %d' % (vtoosmall,T_in)
    chi,tstar_out,tau_out,td_out,thick_out,sluml,r1,vlog,dustm, chi_dLdt,nvarpoints,chi_vlog = dusty_loop(tstarnew,T_in,taunew,thicknew)
    v_list.append(vlog)
    tdust_list.append(T_in)
    if verbose: print '    vlast =', v_list[-1], 'vgoal =', vlog0
    vmin = min(v_list)
    vmax = max(v_list)
    Tmin = min(tdust_list)
    Tmax = max(tdust_list)
    if vtoosmall > 5: break
  # calculate the ideal tdust
  if verbose: print '  first fit'
  tdust_optimized = fit_parabola(v_list,tdust_list,vlog0) # find tdust that yields v_out ~ v_in
  # run dusty one more time with this optimized parameter value
  chi,tstar_out,tau_out,td_out,thick_out,sluml,r1,vlog,dustm, chi_dLdt,nvarpoints,chi_vlog = dusty_loop(tstarnew,tdust_optimized,taunew,thicknew)
  if verbose: print '    chi2 = %s' % (chi)
  z = 1
  while chi_vlog > tol:
    if verbose: print '  extra loop %d' % (z)

    v_list.append(vlog)
    tdust_list.append(tdust_optimized)
    tdust_optimized = fit_parabola(v_list,tdust_list,vlog0) # find tdust that yields v_out ~ v_in
    chi,tstar_out,tau_out,td_out,thick_out,sluml,r1,vlog,dustm, chi_dLdt,nvarpoints,chi_vlog = dusty_loop(tstarnew,tdust_optimized,taunew,thicknew)
    if verbose: print '    chi2 = %s' % (chi)
    z += 1
    if z > maxtries: 
      if verbose: print 'WARNING: chi2 did not converge'
  if verbose and (args.dLdt_limit or args.variability_constraints): print 'chi^2 from dL/dt = %0.1f (%d measurements -> reduced chi^2 = %0.3f)' % (chi_dLdt,nvarpoints,chi_dLdt/nvarpoints)
  return chi, tstar_out, tau_out, td_out, thick_out, sluml, r1, vlog, dustm, chi_dLdt,nvarpoints,chi_vlog

def dusty_loop(tstarnew,tdnew,taunew,thicknew):
  if verbose: print 'tdust = %0.1f' % (tdnew)
  ltau = np.log10(taunew+1.e-10)

  # identify the input stellar spectrum closest to the requested spectrum
  dmin = 1.0e32
  d = abs(tstarnew-tlist)
  imin = np.argmin(d)
  if verbose: print 'will use temperature %s' % (tlist[imin])
  fileuse = 'tempspec.dat'

  # for some reason Kochanek's code takes the closest spectral template that 
  #   is cooler than the star instead of simply the closest overall
  # I'll try following here to see if this accounts for the differences
  #   in results between our codes
  d = tstarnew-tlist
  mask = (d<0)
  d[mask] = d[mask]*(-1e6)
  imin = np.argmin(d)

  # generate the stellar spectrum -- writes it to a temporary file (tempspec.dat)
  # which is then used by DUSTY as the input spectrum
  # keep it in the available range
  tstarnew = max(tlist[0],min(tlist[nlist-1],tstarnew))
  output = open(fileuse,'w')
  slope = (tstarnew-tlist[imin]) / (tlist[imin+1]-tlist[imin])  # this will probably crap out if the matched spectra is the last one...
  if debug: print 'slope = %s, %s %s %s' % (slope,tstarnew,tlist[imin],tlist[imin+1])
  #if slope < 0 or slope > 1:	# -> this was exiting out when the temperature = that of a template, where a slope of -0.000000 was being returned
  if slope < -1.e-10 or slope > 1:
    print 'slope out of range: %f' % (slope)
    print 'wanted %0.1f' % (tstarnew)
    print 'bracket claim is %s,%s' % (tlist[imin],tlist[imin+1])
    exit(1)
  # write out the model spectrum being used
  #  NOTE: Kochanek's dustymc_detection.f cuts the spectra off after 200 lines
  #    but I'll use the whole spectrum because that just seems prudent
  for i in xrange(len(marcslam)):
    output.write('%g\t%g\n' % (marcslam[i], (1.0-slope)*marcsflux[imin,i] + slope*marcsflux[imin+1,i]))
  output.close()

  lam, flx, npt, r1, ierror = geninput(tstarnew,taunew,tdnew,thicknew,idtype,fileuse)

  aa = 0.0
  bb = 0.0
  val = np.zeros(100)
  vall = np.zeros(100)
  for i in xrange(nm):
    j = locate(lam,npt,mlam[i]) - 1   # -1 because indexed to 0
    val[i] = flx[j] + (flx[j+1]-flx[j])*(mlam[i]-lam[j])/(lam[j+1]-lam[j])
    aa = aa + (val[i]/merr[i])**2
    if mlum[i] > 0:
      print 'ERROR: I am expecting only upper limits'
      exit(1)
  #chilim = 4.0 # this is the limit on chi^2
  #slum = np.sqrt(chilim/aa)
  #sluml = np.log10(slum)
  slum = 10**sluml
  if verbose: print 'luminosity ', slum
  # dust radius
  r1 = np.log10(r1) + 0.5*(sluml-4.0)
  # work out the implied mass in msun for kappa=100
  dustm = -34.20 + 2.0*r1 + ltau
  # recompute chi^2 -- should come out as chilim - this is just a sanity check
  chi = 0.0
  chis = {}
  for i in xrange(nm):
    chis[filters[i]] = (slum*val[i]/merr[i])**2
    chi = chi + chis[filters[i]]
  #if verbose: print 'wanted chi^2 = %s, got %s' % (chilim, chi)
  # prior on velocity
  vlog = r1-dlog
  if shell:
    chis['vlog'] = ((vlog-vlog0)/evlog)**2
  else:
    chis['vlog'] = 0
  #chi = chi + chis['vlog']
  if verbose: 
    print '  velocity in', vlog0, ' out', vlog, ' radius ', r1
    print '  log10(ejecta mass) is roughly ', dustm, 'msun'
    print '  chi^2 after velocity prior:', chi
  varlums = []
  nvarpoints = 0
  chi_dLdt = 0
  if (args.variability_constraints or args.dLdt_limit) and not first:
    nvarpoints = 0
    chi_dLdt = 0
    tau_eff = taunew*tau_coeff
    if verbose: print 'tau_tot = %0.3f\ntau_eff = %0.3f' % (taunew, tau_eff)
    for i in xrange(nvar): 
      j = locate(lam,npt,dLdt_lambdas[i]) - 1
      valvar = flx[j] + (flx[j+1]-flx[j])*(dLdt_lambdas[i]-lam[j])/(lam[j+1]-lam[j])
      varfilterlum = 10**sluml * valvar # luminosity in a filter that has a dL/dt measurement
      varlums.append(varfilterlum)
      tau_eff_filter = tau_eff*tau_ratios[i]
      if args.dLdt_limit:
        nvarpoints += 1
        dLdt_model = 2.0*varfilterlum*tau_eff_filter / telapsed_years
        chis['dL_('+args.dLdt_filters[i]+')/dt'] = ( (dLdt_model - args.dLdt_obs[i]) / args.dLdt_obs_errs[i] )**2
        if verbose:
          print '***  %s L_modeled = %0.3g' % (args.dLdt_filters[i],varfilterlum)
          print '***    dL_(%s)/dt observed = %0.3g +/- %0.3g L_sun' % (args.dLdt_filters[i],args.dLdt_obs[i],args.dLdt_obs_errs[i])
          print '***    dL_(%s)/dt model = %0.3g L_sun' % (args.dLdt_filters[i],dLdt_model)
          print '***    chi^2_(dL/dt) = %0.3g' % (chis['dL_('+args.dLdt_filters[i]+')/dt'])
      else:
        # luminosity evolves as L_1 = L_0 e^{-tau[(t_0/t_1)^2-]}
        # luminosity of epochs going into the reference image
        if ref_post_transient:
          modelreflums = varfilterlum * np.exp( -tau_eff * ( (telapsed_years/ref_data[args.dLdt_filters[i]+'_REF_YEARS_ELAPSED'])**2 - 1.0 ) )
        else:
          modelreflums = varfilterlum * np.exp( tau_eff_filter ) # if the ref is from pre-transient I should use the
          #     unobscured luminosity --> this is only in a testing stage...
       # predicted luminosity of the target in the reference image
        modelmean = np.average(modelreflums)
        # predictived luminosity of the target in all of the epochs making up the lightcurve data
        modellums = varfilterlum * np.exp( -tau_eff * ( (telapsed_years/lightcurve_data[args.dLdt_filters[i]]['YEARS_ELAPSED'][years_elapsed_mask])**2 - 1.0 ) )
        modeldiff = modellums - modelmean
        chi_epochs = ( (lightcurve_data[args.dLdt_filters[i]]['L'][years_elapsed_mask] - modeldiff) / lightcurve_data[args.dLdt_filters[i]]['Lerr'][years_elapsed_mask] )**2
        chis['dL_('+args.dLdt_filters[i]+')/dt'] = np.sum( chi_epochs )
        nvarpoints += len(chi_epochs)
        tauepoch = tau_eff_filter * (telapsed_years/lightcurve_data[args.dLdt_filters[i]]['YEARS_ELAPSED'][years_elapsed_mask])**2
        if verbose:
          print '*** %s-band Variability Constraints' % (args.dLdt_filters[i])
          print '  %s L_modeled,now = %0.3g\twith tau_eff,%s = %0.2f' % (args.dLdt_filters[i],varfilterlum,args.dLdt_filters[i],tau_eff_filter)
          print '  %6s\t%18s\t%11s\t%11s\t%10s\t%10s' % ('YRS_ELAP','tau_eff,'+args.dLdt_filters[i],'delta_L_MOD','delta_L_OBS','LERR_OBS','CHI^2')
          for j in xrange(len(lightcurve_data[args.dLdt_filters[i]]['L'][years_elapsed_mask])):
            print '  %6.2f\t%18.2f\t%11.2f\t%11.2f\t%10.2f\t%10.2f' % (lightcurve_data[args.dLdt_filters[i]]['YEARS_ELAPSED'][years_elapsed_mask][j], tauepoch[j], modeldiff[j], lightcurve_data[args.dLdt_filters[i]]['L'][years_elapsed_mask][j], lightcurve_data[args.dLdt_filters[i]]['Lerr'][years_elapsed_mask][j], chi_epochs[j])
      chi_dLdt = chi_dLdt + chis['dL_('+args.dLdt_filters[i]+')/dt']
    #if verbose: print 'chi^2 from dL/dt = %0.1f (%d measurements -> reduced chi^2 = %0.3f)' % (chi_dLdt,nvarpoints,chi_dLdt/nvarpoints)
  #dLdt_model = 2.0*varfilterlum*tau_eff*tau_ratios[i] / telapsed_years
  #this_chi = ( (dLdt_model - args.dLdt_obs[i]) / args.dLdt_obs_errs[i] )**2
  #if verbose:
  #  print '***  %s L_modeled = %0.3g' % (args.dLdt_filters[i],varfilterlum)
  #  print '***    dL_(%s)/dt observed = %0.3g +/- %0.3g L_sun' % (args.dLdt_filters[i],args.dLdt_obs[i],args.dLdt_obs_errs[i])
  #  print '***    dL_(%s)/dt model = %0.3g L_sun' % (args.dLdt_filters[i],dLdt_model)
  #  print '***    chi^2_(dL/dt) = %0.3g' % (this_chi)
  #chi_dLdt = chi_dLdt + this_chi
  return chi,tstarnew,taunew,tdnew,thicknew,sluml,r1,vlog,dustm,chi_dLdt,nvarpoints,chis['vlog']

def date2jd(calendar_date):
  date_s = calendar_date.split('-')
  year = date_s[0]
  month = date_s[1]
  day = date_s[2]
  jd_date = jdcal.gcal2jd(year,month,day)[1]
  return jd_date

#======================================================================
# Initialize Parameters
#======================================================================

parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument("-v", "--verbose", help="increase output verbosity", action="store_true")
parser.add_argument("--nsteps", help="number of stellar temperature steps to evaluate", type=int, default=100)
parser.add_argument("--Tmin", help="minimum stellar temperature", type=float, default=3500)
parser.add_argument("--Tmax", help="maximum stellar temperature", type=float, default=38999)
parser.add_argument("--tau", help='list of taus to evaluate (separated by spaces, e.g., "0 1 3 10")', type=int, nargs='+', default=[0,1,3,10])
parser.add_argument("--idtype", help="dust type: 0=graphite 1=silicate", type=int, choices=[0,1], default=1)
parser.add_argument("--shell", help="model type: 0=wind 1=shell", type=int, choices=[0,1], default=1)
parser.add_argument("--custom_grain_distribution", help="use a custom grain distribution? (if 0 then will use the standard MRN distribution)", type=int, choices=[0,1], default=0)
parser.add_argument("--amin", help="minimum dust size [micron] (used if custom_grain_distribution mode on)", type=float, default=0.005)
parser.add_argument("--amax", help="maximum dust size [micron] (used if custom_grain_distribution mode on)", type=float, default=0.25)
parser.add_argument("--td", help="Dust temperature (if evaluating a wind model)", type=float, default=1500)
parser.add_argument("--thick", help="thickness", type=float, default=2.0)
parser.add_argument("--v0", help="velocity [km/s]", type=float, default=765.0) # SN 1997bs according to Smith et al. 2011
#parser.add_argument("--v0", help="velocity [km/s]", type=float, default=560.0) # NGC300-OT according to Smith et al. 2011
#parser.add_argument("--v0", help="velocity [km/s]", type=float, default=1100.0) # SN 2008S according to Smith et al. 2011
parser.add_argument("--evlog", help="log velocity err", type=float, default=0.3)
parser.add_argument("--start_date", help="date shell ejected [yyyy-mm-dd]", type=str, default='1997-4-15') # SN 1997bs
#parser.add_argument("--start_date", help="date shell ejected [yyyy-mm-dd]", type=str, default='2008-4-24') # NGC 300-OT
#parser.add_argument("--start_date", help="date shell ejected [yyyy-mm-dd]", type=str, default='2008-2-2') # SN 2008S
#parser.add_argument("--start_date", help="date shell ejected [yyyy-mm-dd]", type=str, default='2008-4-24')
#parser.add_argument("--now_date", help="date of observations", type=str, default='2014-8-29')
parser.add_argument("--now_date", help="date of observations", type=str, default='2014-11-29') # SN 1997bs observation
parser.add_argument("--root", help="root name for output", type=str, default='limits')
#parser.add_argument("--path_to_filter_files", help="path to filter transmission and stellar spectra files", type=str, default='../MARCS/')
parser.add_argument("--path_to_filter_files", help="path to filter transmission and stellar spectra files", type=str, default='../../mylimits/MARCS/')
parser.add_argument("--progenitor", help="use progenitor mode? (uses progenitor photometry file)", type=int, choices=[0,1], default=0)
parser.add_argument("--progenitor_photometry_file", help="name of file giving progenitor photometry (used when progenitor mode enabled)", type=str, default='premags.dat')
parser.add_argument("--object_photometry_file", help="name of file giving photometry of the object (used when progenitor mode disabled)", type=str, default='2014_mags_limits.dat')
parser.add_argument("--effective_optical_depth", help="treat input taus as effective rather than total optical depth? 1=yes, 0=no", choices=[0,1], type=int, default=0)
parser.add_argument("--effective_optical_depth_lambda", help="effective wavelength of dLdt_filter [micro-meters]", default=0.55, type=float)
parser.add_argument("--sed", help="give root name of output to generate SED with plot.sm")
#parser.add_argument("--dLdt_limit", help="calculate a (separate) chi^2 from dL/dt measurements", action="store_true")
parser.add_argument("--dLdt_filters", help="list of filters to apply dL/dt limits to (if using dLdt_limit option)", nargs='+', type=str)
#parser.add_argument("--dLdt_obs", help="observed dL/dt's [L_sun/yr] (if using dLdt_limit option)", nargs='+', type=float)
#parser.add_argument("--dLdt_obs_errs", help="observed dL/dt 1-sigma uncertainties [L_sun/yr] (if using dLdt_limit option)", nargs='+', type=float)
parser.add_argument("--variability_constraints", help="use variability constraints from light curve(s)?", action="store_true")
parser.add_argument("--variability_files", help="list of files containing calibrated light curves (with 'L' and 'Lerr' in L_sun and 'DATE' in JD)", nargs="+")
parser.add_argument("--vardateoffset", help="days to subtract off lightcurve epochs to set to JD-2400000", type=float, choices=[0,2400000], default=0)
parser.add_argument("--ref_lists", help="names of ref_list files used to generate reference images used by light curves", nargs="+")
parser.add_argument("--min_years_elapsed", help="minimum years since event to start using lightcurve constraint", type=float, default=2.0)
#parser.add_argument("--chilims", help="list of chi^2_photlimits to use", nargs="+", type=float, default=[1,4,9])
parser.add_argument("--minluml", help="minimum log(L) to evaluate", type=float, default=3.0)
parser.add_argument("--maxluml", help="minimum log(L) to evaluate", type=float, default=6.5)
parser.add_argument("--nlum", help="number of luminosity steps to evaluate", type=int, default=100)
parser.add_argument("--dLdt_limit", help="use a limit on dL/dt?", type=int, choices=[0,1], default=0)
parser.add_argument("--dLdt_obs", help="observed dL/dt's [L_sun/yr] (if using dLdt_limit option)", nargs='+', type=float, default=0)
parser.add_argument("--dLdt_obs_errs", help="observed dL/dt 1-sigma uncertainties [L_sun/yr] (if using dLdt_limit option)", nargs='+', type=float, default=0)
args = parser.parse_args()

debug = 0

verbose = args.verbose
nsteps = args.nsteps
Tmin = args.Tmin
Tmax = args.Tmax
tau = args.tau
idtype = args.idtype
shell = args.shell
custom_grain_distribution = args.custom_grain_distribution
amin = args.amin
amax = args.amax
td = args.td
thick = args.thick
v0 = args.v0
evlog = args.evlog
start_date = args.start_date
now_date = args.now_date
root = args.root
path_to_filter_files = args.path_to_filter_files
progenitor = args.progenitor
#dLdt_limit = args.dLdt_limit
#chilims = args.chilims

tstart = date2jd(args.start_date) + args.vardateoffset
tnow = date2jd(args.now_date) + args.vardateoffset
telapsed = tnow-tstart
telapsed_years = telapsed / 365.25

if verbose: print 'elapsed time %s days' % (telapsed)

# coefficient 8.64e09 is (1 km/s)(day) so dlog = log(distance moved given time baseline at 1km/s)
dlog = np.log10(8.640e9*telapsed)
if verbose: print 'dlog = %0.2f' % (dlog)

#Tstar = np.logspace(np.log10(Tmin),np.log10(Tmax),nsteps)
Tstar = np.linspace(Tmin,Tmax,nsteps)
vlog0 = np.log10(v0)

#----------------------------------------------------------------------

if progenitor:
  bestfile = 'prog'
  photometry_file = args.progenitor_photometry_file
else:
  bestfile = 'best'
  photometry_file = args.object_photometry_file

# NOTE: I had the dust_type labels reversed prior to 6 Nov 2014
if idtype == 1:
  dust_type = 'silicate'
else:
  dust_type = 'graphite'

if shell == 1:
  shell_or_wind = 'shell'
else:
  shell_or_wind = 'wind'

#======================================================================
# read in the stellar spectra
nlist, tlist, flist = readkurucz()
marcsflux = np.zeros([nlist,417]) # I'm assuming all of the spectra files have 417 lines
for i in xrange(nlist):
  marcslam, marcsflux1 = np.loadtxt(flist[i], usecols=(0,1), unpack=True)
  try:
    marcsflux[i,:] = marcsflux1
  except:
    print 'ERROR: this program is hard-coded to expect spectra file with 417 lines only'
    print '  %s has %d lines' % (flist[i], len(marcsflux1))
    exit(1)
if verbose: print 'done reading in model spectra'

# read in the photometry file (magnitudes, filters)
data = ascii.read(photometry_file)
mlam = data['col1']
mlum = data['col2']
mluml = np.zeros(len(mlum))
merrl = np.zeros(len(mlum))
gtzero = (mlum > 0)
ltzero = np.invert(gtzero)
mluml[gtzero] = np.log10(mlum[gtzero])
merr = data['col3']
merrl[gtzero] = merr[gtzero]/mlum[gtzero]/np.log(10)
merrl[ltzero] = np.log10(merr[ltzero])
filters = data['col4']
templam, trans = read_filters(filters)
nm = np.shape(mlum)[0]
if verbose: print 'read %d data points' % (nm)

#----------------------------------------------------------------------
# Other one-time things
if args.variability_constraints or args.dLdt_limit:
  nvar = len(args.dLdt_filters)
  dLdt_lambdas = filter_effective_wavelength(args.dLdt_filters)

# Read in Variability data
if args.variability_constraints:
  lightcurve_data = {}
  ref_data = {}
  for i in xrange(len(args.dLdt_filters)):
    filter = args.dLdt_filters[i]
    lightcurve_data[filter] = ascii.read(args.variability_files[i],delimiter=',')
    lightcurve_data[filter]['L'] = lightcurve_data[filter]['L'] * -1    # ISIS has the sign backwards...
    lightcurve_data[filter]['YEARS_ELAPSED'] = (lightcurve_data[filter]['DATE'] - tstart) / 365.25
    years_elapsed_mask = (lightcurve_data[filter]['YEARS_ELAPSED'] > args.min_years_elapsed)
    if verbose: print 'lightcurve points before trim:', len(lightcurve_data[filter]['YEARS_ELAPSED'])
    if verbose: print 'lightcurve points after trim:', len(lightcurve_data[filter]['YEARS_ELAPSED'][years_elapsed_mask])
    # read in ref_list's -- images used to construct the reference images    
    ref_data[filter+'_REF_DATES'] = np.loadtxt(args.ref_lists[i], usecols=(1,2), unpack=True)[0]
    ref_data[filter+'_REF_YEARS_ELAPSED'] = (ref_data[filter+'_REF_DATES'] - tstart) / 365.25
    if verbose: print 'reference years elapsed:', ref_data[filter+'_REF_YEARS_ELAPSED']
    mask = (ref_data[filter+'_REF_YEARS_ELAPSED'] > 0)
    fpostevent = float(len(ref_data[filter+'_REF_YEARS_ELAPSED'][mask])) / float(len(ref_data[filter+'_REF_YEARS_ELAPSED']))
    mask = (ref_data[filter+'_REF_YEARS_ELAPSED'] > args.min_years_elapsed)
    fpostminyearselapsed = float(len(ref_data[filter+'_REF_YEARS_ELAPSED'][mask])) / float(len(ref_data[filter+'_REF_YEARS_ELAPSED']))
    if fpostevent == 1 and fpostminyearselapsed == 1:
      ref_post_transient = 1
    elif fpostevent == 0:
      ref_post_transient = 0
      print 'WARNING: ref from pre-event imaging -- this scenario has not been tested yet...'
    else:
      print 'ERROR: %s is a mixture of pre and post event imaging -- this is not currently supported' % (args.ref_lists[i])
      exit(1)
    if verbose: print 'ref_post_transient =', ref_post_transient

luminosities = np.linspace(args.minluml,args.maxluml,args.nlum)
sluml = luminosities[0]

# run dusty a first time to figure out the tau_eff_coeff
first = 1
#chilim = 4
dusty_loop(Tstar[0],td,1,thick)
tau_coeff, tau_ratios = calculate_tau_coeff()
first = 0

for k in xrange(len(tau)):
  if args.effective_optical_depth:
    output = open(root+'_grid_taueff'+str(int(tau[k]))+'.dat','w')
    tau[k] = float(tau[k]) / tau_coeff
  else:
    output = open(root+'_grid_tau'+str(int(tau[k]))+'.dat','w')
  if verbose: print 'will run DUSTY for tau=%s' % tau[k]
  output.write("# Generated using '%s' %s\n" % (''.join("%s " % ''.join(map(str, x)) for x in argv), datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
  if shell == 0: output.write('# Tdust = %s\n' % (td))
  output.write('# shell thickness = %s (R_out/R_in)\n# silicate = %s (0=graphite 1=silicate)\n# v = %s [km/s]\n# verr = %s [log error]\n# date_peak = %s\n# date_obs = %s\n# dust structure = %s\n' % (thick,idtype,v0,evlog,tstart,tnow,shell_or_wind))
  if custom_grain_distribution:
    output.write('# custom_grain_size_distribution = 1\n#   q = 3.5\n#   a_min = %s %% micron\n#   a_max = %s %% micron\n' % (amin,amax))
  else:
    output.write('# custom_grain_size_distribution = 0 % (standard MRN distribution)\n#   q = 3.5\n#   a_min = 0.005 % [micron]\n#   a_max = 0.25 % [micron]\n')
  output.write('# photometry_file = %s\n' % photometry_file)
  for i in xrange(len(mlam)):
    output.write('#   filters%d = %s %s %s %s\n' % (i, mlam[i], mlum[i], merr[i], filters[i]))
  for m in xrange(len(luminosities)):
    sluml = luminosities[m]
    for j in xrange(len(Tstar)):
      if verbose: print 'Run %d: starting with Tstar = %dK, tau = %s, luml = %0.2f' % (j,Tstar[j], tau[k], sluml)
      if shell == 0: chi,tstar_out,tau_out,td,thick_out,lum,rdust,v_out,mass,chi_dLdt,nvarpoints,chi_vlog = dusty_loop(Tstar[j],td,tau[k],thick)
      else: chi, tstar_out, tau_out, td, thick_out, lum, rdust, v_out, mass, chi_dLdt,nvarpoints,chi_vlog = optimize_tdust(Tstar[j],tau[k],thick)
      if m == 0 and j == 0:
        if args.effective_optical_depth:
          output.write('# tau_eff = 1 %% tau in filename refers to effective (rather than total) optical depth\n')
        else:
          output.write('# tau_eff = 0 %% tau in filename refers to total optical depth\n')
        output.write('# tau_eff_coeff = %0.4f %% calculated from albedo for %0.4f micro-meters\n' % (tau_coeff, args.effective_optical_depth_lambda))
        if shell == 0: output.write('%s,%s,%s,%s,%s,%s,%s,%s,%s,%s\n' % ('index','tstar_in','chi2','tstar','tau','tdust','thick','lum','rdust','v_out'))
        else: output.write('%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s\n' % ('index','tstar_in','chi2','tstar','tau','tdust','thick','lum','rdust','v_out','mass','chi_dLdt','nvarpoints','chi_vlog'))
      if verbose == 1 and shell == 1: print 'Mass =', mass
      if shell == 0: output.write('%s,%s,%s,%s,%s,%s,%s,%s,%s,%s\n' % (j,Tstar[j],chi,tstar_out,tau[k],td,thick,lum,rdust,v_out))
      else: output.write('%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s\n' % (j,Tstar[j],chi,tstar_out,tau[k],td,thick,lum,rdust,v_out,mass,chi_dLdt,nvarpoints,chi_vlog))
  output.close()
  if args.sed:
    sedoutput = open(args.sed+'chi'+'.dat','w')
    sedoutput.write('%s\t%s\t%s\t%s\t%s\t%s\t%s\n' % (chi, tstar_out, tau[k], td, thick, lum, rdust))
    sedoutput.close()
    system('cp foo1.inp '+args.sed+'.inp')
    system('cp foo1.itb '+args.sed+'.itb')
    system('cp foo1.out '+args.sed+'.out')
    system('cp foo1.rtb '+args.sed+'.rtb')
    system('cp foo1.spp '+args.sed+'.spp')
    system('cp foo1.stb '+args.sed+'.stb')
    #if dLdt_limit:
    #  sedoutput = open(args.sed+'var.dat','w')
    #  for i in xrange(len(varlums)):
    #    sedoutput.write('%s\t%0.3f\t%0.3f\n' % (args.dLdt_filters[i], dLdt_lambdas[i], varlums[i]))
    #  sedoutput.close()
